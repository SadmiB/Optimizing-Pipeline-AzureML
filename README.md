# Optimizing an ML Pipeline in Azure
 
## Overview
This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run.
 
## Summary
**In 1-2 sentences, explain the problem statement:**
 
This dataset contains data about bank customers including their job, marital status, housing, salary..etc. The primary goal is to predict if a specific person is eligible to take a loan or not. Which means that this is a classification problem with two classes, for this we will be using hyperdrive and automl and compare their performance.
![Pipeline](assets/pipeline.png) 
**In 1-2 sentences, explain the solution:**
 
The best model selected by AutoML reached an **accuracy of 0.912959028831563** and **AUC weighted of 0.9470720159066244** and **the algorithm MaxAbsScaler, LightGBM**.
 
## Scikit-learn Pipeline
**Explain the pipeline architecture, including data, hyperparameter tuning, and classification algorithm.**
 
The scikit-learn pipeline uses SKLearn estimator that runs the script train.py, the script is responsible for loading the bank marketing dataset and do some data cleansing, we replace job, education, contact with dummy data, and we replace categorical data with integer encoding. Lastly we return the features and the labels “y”.
 
The script uses **Logistic Regression** as the classification algorithm.
 
We tuning two hyperparameters which are **regularization** and **maximum number of iterations**, as the problem is a classification problem so I'm using **accuracy** as the primary metric. I set up the pipeline to run a maximum of 16 runs with a maximum of 4 in parallel.
 
 
I used Random Sampling which performs almost as the Grid Sampling and it necessits less time and resources. For early stopping I'm using Bandit Policy so we terminate runs that don't perform well, in our case we evaluate the runs each 100 iteration and discard runs covered by the slack_mount of 0.02.
 
 
**What are the benefits of the parameter sampler you chose?**
The random parameter sampler supports discrete values and values distributed over a continuous range, furthermore it gives almost the same performance as Grid sampling which consumes more time.
 
**What are the benefits of the early stopping policy you chose?**
Bandit is an early termination policy based on slack factor/slack amount and evaluation interval. The policy early terminates any runs where the primary metric is not within the specified slack factor/slack amount with respect to the best performing training run.
 
As explained in [microsoft documentation](https://docs.microsoft.com/en-us/python/api/azureml-train-core/azureml.train.hyperdrive.banditpolicy?view=azure-ml-py):
 
Consider a Bandit policy with slack_factor = 0.2 and evaluation_interval = 100. Assume that run X is the currently best performing run with an AUC (performance metric) of 0.8 after 100 intervals. Further, assume the best AUC reported for a run is Y. This policy compares the value (Y + Y * 0.2) to 0.8, and if smaller, cancels the run. If delay_evaluation = 200, then the first time the policy will be applied is at interval 200.
 
the best performing model reached an **accuracy of  0.9119916760600018** with the hyperparameters of **max iterations equal 50** and **inverse regularization equal 50(Regularization equal 0.02)**.
 
![Hyperdrive accuracy](assets/hyperdrive_accuracy.png)
 
## AutoML
**In 1-2 sentences, describe the model and hyperparameters generated by AutoML.**
The best model selected by AutoML reached an **accuracy of 0.91351**, **AUC weighted of 0.94866**, **F1 score micro of 0.91351** and **the algorithm VotingEnsemble**, the hyperparameters generated by AutoML are **max_iter=100, multi_class='multinomial', n_jobs=1, penalty='l2', solver='newton-cg' and the Y_tranfomer='LabelEncoder'**.
 
 
![Confusion matrix](assets/confusion_matrix.png)
 
## Pipeline comparison
**Compare the two models and their performance. What are the differences in accuracy? In architecture? If there was a difference, why do you think there was one?**
 
AutoML tests many configurations and algorithms at the same time, but hyperdrive is limited to the configuration we provide.
 
The results achieved by AutoML are slightly better than the results achieved by scikit-learn pipeline using hyperdrive. Also I find the AutoML pipeline more easy to set up and it takes away a lot of configurations from the data scientist, on the other side the hyperdrive requires more work to do. Also the AutoML tests many algorithms without any further configuration but with hyperdrive you should explicitly test every algorithm.
 
AutoML also analyses different metrics and you will be able to choose the primary metric that fits for your problem most, but in hyperdrive this is done manually for every metric.
 
Also when activating featurization AutoML detects that the dataset has imbalanced classes, so for that using AUC-weighted helps dealing with the issue.
 
With hyperdrive the best performing model reached an **accuracy of  0.9119916760600018** with the hyperparameters of **max iterations equal 50** and **inverse regularization equal 50(Regularization equal 0.02)**.
 
With AutoML the best model reached an **accuracy of 0.91351**, **AUC weighted of 0.94866**, **F1 score micro of 0.91351** and **the algorithm VotingEnsemble**
 
 
## Future work
**What are some areas of improvement for future experiments? Why might these improvements help the model?**
Try to explore more hyperparameters tuning like the solver and penalty to explore more configuration for hyperdrive which could lead to a better performance, as the dataset is imbalanced so choosing another primary metric could give better results.
 
We use an AUC-weighted primary metric for AutoML as the dataset is imbalanced, but balancing the dataset may give better performance, so this is a good area to explore.
 
## Proof of cluster clean up
Added compute_target.delete() at the end of the notebook.
 
**Image of cluster marked for deletion**
 
![Deleting the cluster](assets/clean_ws.png)
 

