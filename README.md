# Optimizing an ML Pipeline in Azure

## Overview
This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run.

## Summary
**In 1-2 sentences, explain the problem statement:**

This datatset contains data about bank customers including their job, marital status, housing, salary..etc. The primary goal is to predict if a spcefic person is eligible to take a loan or not. Which mean that this is a calassifcation problem with two classes, for this we will be using hyperdrive and automl and compare their performance.

**In 1-2 sentences, explain the solution:**

The best model selected by AutoML reached an **accuracy of 0.912959028831563** and **AUC weighted of 0.9470720159066244** and **the algorithm MaxAbsScaler, LightGBM**.

## Scikit-learn Pipeline
**Explain the pipeline architecture, including data, hyperparameter tuning, and classification algorithm.**

For the scickit-learn pipeline I used Random Sampling which performace almost as the Grid Sampling and it necessits less time and ressources. For early stopping I'm using Bandit Policy so we terminate runs that don't perform well, in our case we evaluate the runs each 100 iteration and discard runs covred by the slack_mount of 0.02.

We tuning two hyperparameters which are **regularization** and **maximum number of iterations**, as the problem is a clasification problem so I'm using **accuracy** as primary metric. For the calssification algorithm we using **Logistic Regression**. 

I setup the pipeline to run a maximum of 16 runs with maximum of 4 in parallel.


**What are the benefits of the parameter sampler you chose?**
The random parameter sampler supports discrete values and values distributed over a continuous range, furthermore it gives almost the same performance as Grid sampling which consumes more time.

**What are the benefits of the early stopping policy you chose?**
Bandit is an early termination policy based on slack factor/slack amount and evaluation interval. The policy early terminates any runs where the primary metric is not within the specified slack factor/slack amount with respect to the best performing training run.

As explained in [microsoft documentation](https://docs.microsoft.com/en-us/python/api/azureml-train-core/azureml.train.hyperdrive.banditpolicy?view=azure-ml-py):

Consider a Bandit policy with slack_factor = 0.2 and evaluation_interval = 100. Assume that run X is the currently best performing run with an AUC (performance metric) of 0.8 after 100 intervals. Further, assume the best AUC reported for a run is Y. This policy compares the value (Y + Y * 0.2) to 0.8, and if smaller, cancels the run. If delay_evaluation = 200, then the first time the policy will be applied is at interval 200.

the best performing model reached an **accuracy of  0.9119916760600018** with the hyperparameters of **max iterations eqaul 50** and **inverse regularization equal 50(Regularization equal 0.02)**.

![Hyperdrive accuracy](assets/hyperdrive_accuracy.png)

## AutoML
**In 1-2 sentences, describe the model and hyperparameters generated by AutoML.**
The best model selected by AutoML reached an **accuracy of 0.912959028831563** and **AUC weighted of 0.9470720159066244** and **the algorithm MaxAbsScaler, LightGBM**.

![Confusion matrix](assets/confusion_matrix.png)

## Pipeline comparison
**Compare the two models and their performance. What are the differences in accuracy? In architecture? If there was a difference, why do you think there was one?**

AutoML tests many configurations and algorithms at the same time, but hyperdrive is limited to the configuration we provide.

The results achieved by AutoML are slighlty better than the results achieved by sckiit-learn pipeline using hyperdrive. Also I find the AutoML pipeline more easy to setup and it takes away a lot of configurations from the data scientist, in the other side the hyperdrive necessists more work to do. Also the AutoML tests many algorithms without any further configuration but with hyperdrive you should explicitly test every algorithm.

AutoML also analyses different metrics and you will be able to chose the primary metric that fits for your problem most, but in hyperdrive this is done manually for every metric. 

Also when activating featurization AutoML detects that the dataset has an imbalanced classes, so for that using AUC-weighted helps dealing with the issue.

With hyperdrive the best performing model reached an **accuracy of  0.9119916760600018** with the hyperparameters of **max iterations eqaul 50** and **inverse regularization equal 50(Regularization equal 0.02)**.

With AutoML the best model selected by AutoML reached an **accuracy of 0.912959028831563** and **AUC weighted of 0.9470720159066244** and **the algorithm MaxAbsScaler, LightGBM**.


## Future work
**What are some areas of improvement for future experiments? Why might these improvements help the model?**
Try to explore more hyperparameters tunning like the solver and penalty to explore more configuration for hyperdrive which could lead to a better performance, as the dataset is imbalanced so chosing another primary metric could give better results. 

We using a AUC-weighted primary metric for AutoML as the dataset is imbalanced, but balancing the dataset may give better performance, so this is a good area to explore.

## Proof of cluster clean up
Added compute_target.delete() at the end of the notebook.

**Image of cluster marked for deletion**

![Deleting the cluster](assets/clean_ws.png)